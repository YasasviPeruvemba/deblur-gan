{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "DeblurGAN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6gaLXXgj26L"
      },
      "source": [
        "!git clone https://github.com/YasasviPeruvemba/deblur-gan.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKhvuyszkKVG"
      },
      "source": [
        "%%bash\n",
        "mkdir /content/deblur-gan/gopro/train -p\n",
        "mkdir /content/deblur-gan/logs\n",
        "# Find Dataset from here - https://drive.google.com/file/d/1CPMBmRj-jBDO2ax4CxkBs9iczIFrs8VA/view?usp=sharing\n",
        "gdown --id \"1CPMBmRj-jBDO2ax4CxkBs9iczIFrs8VA\" \n",
        "mv blurred_sharp.zip /content/deblur-gan/gopro/train/\n",
        "cd /content/deblur-gan/gopro/train\n",
        "unzip blurred_sharp.zip -q\n",
        "mv \"./blurred_sharp/blurred\" \"./blurred_sharp/blur\"\n",
        "rm -r /content/deblur-gan/gopro/train/__MACOSX\n",
        "rm blurred_sharp.zip\n",
        "cd /content/deblur-gan"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-ZzgG_Xn1OM"
      },
      "source": [
        "%cd /content/deblur-gan/\n",
        "%pip install -r requirements/requirements.txt\n",
        "%pip install -e ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFLVZehBkrRw",
        "outputId": "a6613c56-ccde-475d-ab01-0ebe88fd197b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python scripts/organize_gopro_dataset.py --dir_in=./gopro --dir_out=./images"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dir:   0% 0/1 [00:00<?, ?it/s]\n",
            "image_folders:   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "image_folders: 100% 1/1 [00:03<00:00,  3.88s/it]\n",
            "dir: 100% 1/1 [00:03<00:00,  3.88s/it]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IP2ZMcebpZ2i"
      },
      "source": [
        "# CHECK IF THE PREPROCESSING WORKED\n",
        "import matplotlib.pyplot as plt\n",
        "a = plt.imread(\"./images/train/A/blurred_sharp_257.png\")\n",
        "plt.imshow(a);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50HUCRj_4oeG"
      },
      "source": [
        "!python scripts/deblur_image.py --weight_path=/content/deblur-gan/generator_195_204.h5 --input_dir=./images/ --output_dir=./results/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lX5BPIrU7Ie2"
      },
      "source": [
        "# !rm -rf custom_test/input/.ipynb_checkpoints/* custom_test/output/.ipynb_checkpoints/* custom_test/.ipynb_checkpoints/*\n",
        "# !rmdir custom_test/input/.ipynb_checkpoints custom_test/output/.ipynb_checkpoints custom_test/.ipynb_checkpoints"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxXRROnisw6Z"
      },
      "source": [
        "# Gradients with respect to images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zCDd_5BrE7s"
      },
      "source": [
        "import os\n",
        "import datetime\n",
        "import click\n",
        "import numpy as np\n",
        "import tqdm\n",
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "\n",
        "from deblurgan.utils import load_images, write_log\n",
        "from deblurgan.losses import wasserstein_loss, perceptual_loss_100\n",
        "from deblurgan.model import generator_model, discriminator_model, generator_containing_discriminator_multiple_outputs\n",
        "\n",
        "import numpy as np"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYakqDWAPn66"
      },
      "source": [
        "def get_gen_grads(batch_size=2):\n",
        "    data = load_images('./images/train', batch_size)\n",
        "    y_train, x_train = data['B'], data['A']\n",
        "\n",
        "    g = generator_model()\n",
        "    g.load_weights('generator_195_204.h5')\n",
        "    d = discriminator_model()\n",
        "    d.load_weights('discriminator_195.h5')\n",
        "\n",
        "    model = generator_containing_discriminator_multiple_outputs(g, d)\n",
        "\n",
        "    sess = K.get_session()\n",
        "    perceptual_loss_func = tf.gradients(perceptual_loss_100(tf.convert_to_tensor(y_train), model.output[0]), model.input)\n",
        "    wasserstein_loss_func = tf.gradients(wasserstein_loss(tf.convert_to_tensor(np.ones(y_train.shape[0], dtype=np.float32)), model.output[1]), model.input)\n",
        "    gen_gradients = sess.run([perceptual_loss_func, wasserstein_loss_func], feed_dict={model.input : x_train})\n",
        "\n",
        "    print(x_train.shape)\n",
        "    print(gen_gradients[0][0].shape)\n",
        "    print(gen_gradients[1][0].shape)\n",
        "\n",
        "# Send batch_size small as it is computationally heavy\n",
        "get_gen_grads(batch_size=2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xA6d-Jt0i2NR"
      },
      "source": [
        "def get_discrimator_grads(batch_size=2):\n",
        "    data = load_images('./images/train', batch_size)\n",
        "    y_train, x_train = data['B'], data['A']\n",
        "\n",
        "    model = discriminator_model()\n",
        "    model.load_weights('discriminator_195.h5')\n",
        "\n",
        "    ground_truths_real = tf.convert_to_tensor(np.ones((batch_size, 1), dtype=np.float32))\n",
        "    ground_truths_fake = tf.convert_to_tensor(np.zeros((batch_size, 1), dtype=np.float32))\n",
        "\n",
        "    ground_truth = tf.concat([ground_truths_real, ground_truths_fake], axis=0)[0]\n",
        "    input = np.append(y_train, x_train, axis=0)\n",
        "\n",
        "    sess = K.get_session()\n",
        "    grad_func = tf.gradients(wasserstein_loss(ground_truth, model.output), model.input)\n",
        "    disc_gradients = sess.run(grad_func, feed_dict={model.input : input})[0]\n",
        "\n",
        "    print(input.shape)\n",
        "    print(disc_gradients.shape)\n",
        "    print(disc_gradients)\n",
        "\n",
        "# Send batch_size small as it is computationally heavy\n",
        "get_discrimator_grads(batch_size=2)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
